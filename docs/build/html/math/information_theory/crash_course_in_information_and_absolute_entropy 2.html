
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Crash Course in Information and Absolute Entropy &#8212; Jeremy Mann&#39;s Writing 0.0.0 documentation</title>
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Crash Course in Relative Entropy I: Mutual Information" href="crash_course_in_relative_entropy_i_mutual_information.html" />
    <link rel="prev" title="Information Theory" href="information_theory_main_page.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="crash-course-in-information-and-absolute-entropy">
<h1>Crash Course in Information and Absolute Entropy<a class="headerlink" href="#crash-course-in-information-and-absolute-entropy" title="Permalink to this headline">¶</a></h1>
<p>The goal of these notes is to give a minimal introduction to the notions of information and (absolute) entropy.</p>
<p>Although these notes have an expository tone, the definition admonitions provide precise definitions.</p>
<div class="section" id="information">
<h2>Information<a class="headerlink" href="#information" title="Permalink to this headline">¶</a></h2>
<p>Gibbs and Shannon taught us that the likelihood of an event (according to a hypothesis) is encapsulated in the amount of useful information (according to a hypothesis) present it’s observation. Heuristically, “the information in an event scales with the amount of surprise upon it’s observation.”</p>
<p>Let’s imagine you’re computing information in bits, and <span class="math notranslate nohighlight">\(\rho\)</span> was the uniform distribution on binary sequences of length <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p>In this case, the information contained in a binary single string <span class="math notranslate nohighlight">\(S\)</span> coincides with it’s length. This happens to the be:</p>
<div class="math notranslate nohighlight">
\[\mathcal{I}_\rho(S) = \log_2((.5^n)^{-1}) = -\log_2{(.5^n)} = n\]</div>
<p>For example, “010” contains 3 bits of information</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>Given a fixed probability distribution, with density <span class="math notranslate nohighlight">\(\rho\)</span>, with sample space <span class="math notranslate nohighlight">\(\mathscr{X}\)</span>. For any <span class="math notranslate nohighlight">\(x \in \mathscr{X}\)</span>, the <strong>information</strong> in x is:</p>
<div class="math notranslate nohighlight">
\[\mathcal{I}_{\rho}(x) = \log\Bigl(\frac{1}{p(x)}\Bigl) = - \log(p(x))\]</div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>We will be abusive with which base we take our logarithm with respect to. The choice of a base can be thought of choice of units, and can be recovered by examining the information of obtaining a heads in a fair coin toss.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One should always remember that the notion of information depends explicitly on the probability distribution. Information is very “context-dependent.”</p>
</div>
<p>Below is a plot of information of an event as a function of it’s likelihood:</p>
<img alt="../../_images/probability_vs_information.png" src="../../_images/probability_vs_information.png" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While an event with probability 1 has no information, an event with probability 0 has an infinite amount of information.</p>
<p>If obtaining information IRL requires the expenditure of energy, this provides an articulation of an event being “impossible” in the same sense it is “impossible” to go faster than the speed of light.</p>
</div>
<p>Heuristically, information is dual to likelihood/probability:</p>
<ul class="simple">
<li><dl class="simple">
<dt>High likelihoood events have litle (useful!) information.</dt><dd><ul>
<li><p>Knowing it’s not going to rain next saturday has little useful information if you live in a desert.</p></li>
<li><p><span class="math notranslate nohighlight">\(2 + 2 = 4\)</span> (which we’re certain is true) has zero information.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Low likelihood events have a lot of (useful!) information.</dt><dd><ul>
<li><p>Knowing it’s going to be sunny next saturday has a lot of useful information if you live in Seattle.</p></li>
<li><p><span class="math notranslate nohighlight">\(2 + 2 \neq 4\)</span> has an infinite amount of information.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="entropy">
<h2>Entropy<a class="headerlink" href="#entropy" title="Permalink to this headline">¶</a></h2>
<p>Information is a pairing between events and probability distributions. More formally, it’s a function:</p>
<div class="math notranslate nohighlight">
\[\mathrm{Prob}(\mathcal{X}) \times \mathcal{X} \longrightarrow \mathbb{R}\]</div>
<p>In other words, it gives a random variable for each probability distribution <span class="math notranslate nohighlight">\(\rho\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathcal{X} \overset{\mathcal{I}_{\rho}}\longrightarrow \mathbb{R}\]</div>
<p>whose expectation value provides a numerical invariant of <span class="math notranslate nohighlight">\(\rho\)</span>, namely it’s entropy.</p>
<p>We briefly pause to introduce some</p>
<div class="admonition-notation admonition">
<p class="admonition-title">Notation</p>
<p>Given a random variable <span class="math notranslate nohighlight">\(X\)</span> and a probability distribution <span class="math notranslate nohighlight">\(\rho\)</span>, we let</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_\rho[X] = \langle X \rangle_\rho\]</div>
<p>denote the expectation value of <span class="math notranslate nohighlight">\(X\)</span> with respect to the distribution <span class="math notranslate nohighlight">\(\rho\)</span></p>
</div>
<p>We can now state the</p>
</div>
<div class="section" id="definition-of-entropy">
<h2>Definition of Entropy<a class="headerlink" href="#definition-of-entropy" title="Permalink to this headline">¶</a></h2>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>The (absolute) entropy of <span class="math notranslate nohighlight">\(\rho\)</span> is just the expected amount of information <span class="math notranslate nohighlight">\(\rho\)</span> contains:</p>
<div class="math notranslate nohighlight">
\[\mathcal{S}(\rho) =  \langle  \mathcal{I}_{\rho} \rangle_{\rho}\]</div>
</div>
<p>Below is the a graph of a the entropy of biased coin toss as function of the probability of heads:</p>
<img alt="../../_images/entropy_of_biased_coin.png" src="../../_images/entropy_of_biased_coin.png" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Entropy is maximized when the coin is fair!</p>
<p>Entropy is minimized when the coin has no randomness!</p>
<p>This is the first indication that we can think of entropy as quantifying the amount of uncertainty in a system.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>An essential feature of this definitions is that <span class="math notranslate nohighlight">\(x\)</span> goes to 0 much faster than <span class="math notranslate nohighlight">\(\log(x)\)</span>. In other words,</p>
<div class="math notranslate nohighlight">
\[\lim_{p \rightarrow 0^+} \bigl(- p \log(p) \bigl) = 0\]</div>
<p>This ensures that impossible events do not cause the entropy to be infinite.</p>
</div>
</div>
<div class="section" id="entropy-of-a-normal-distribution">
<h2>Entropy of a Normal Distribution<a class="headerlink" href="#entropy-of-a-normal-distribution" title="Permalink to this headline">¶</a></h2>
<p>The amount of information in a univariate normal distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma)\)</span>, is easy to compute:</p>
<div class="math notranslate nohighlight">
\[\mathcal{I}_{\mathcal{N}(\mu, \sigma)}(x) = \frac{1}{2} \Bigl(\frac{x - \mu}{\sigma}\Bigl)^2 - \log( \sigma\sqrt{2 \pi})\]</div>
<p>This gives a gorgeous description of it’s entropy:</p>
<div class="math notranslate nohighlight">
\[\mathcal{S}\bigl(\mathcal{N}(\mu, \sigma^2) \bigl)= \log ( \sigma) + \log ( \sqrt{2e\pi})\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We see further evidence for the heuristic that entropy quantifies the randomness of the distribution, as it increase monotonically with the standard deviation.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \sigma}\mathcal{S}\bigl(\mathcal{N}(\mu, \sigma^2) \bigl) = \frac{1}{\sigma} &gt; 0\]</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In general, entropy is translation/permutation invariant. Fortunately, it is not dialation invariant.</p>
</div>
<p>The formula for a multivariate Gaussian is analagous but more complex. What’s most interesting is a manifestation of the curse of dimensionality:</p>
<div class="math notranslate nohighlight">
\[\mathcal{S}\bigl(\mathcal{N}(0, \mathbb{1}_n) \Bigl)= n \cdot \mathcal{S}\big(\mathcal{N}(0, 1) \bigl)\]</div>
<p>So that the entropy of a unit normal scales linearly with the dimension.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A general fact is that normal distributions maximize the entropy amongst all distributions with a fixed mean and variance.</p>
<p>More generally, all exponential families (e.g. Boltzman distributions, exponential, multinomial, …) arise by a similar “Maximum Entropy Principle.”</p>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Jeremy Mann's Writing</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../learning/learning_main_page.html">Learning</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../math_main_page.html">Math</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../category_theory/category_theory_main_page.html">Category Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homotopy_theory/homotopy_theory_main_page.html">Homotopy Theory</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="information_theory_main_page.html">Information Theory</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Crash Course in Information and Absolute Entropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="crash_course_in_relative_entropy_i_mutual_information.html">Crash Course in Relative Entropy I: Mutual Information</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/probability_theory_main_page.html">Probability Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../geometry/geometry_main_page.html">Geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/linear_algebra_main_page.html">Linear Algebra</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/physics_main_page.html">Physics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../longevity/longevity_main_page.html">Longevity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/software_main_page.html">Software Stuff</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stream/stream_main_page.html">Stream Main Page</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../to_do.html">To Do List</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../random/random_main_page.html">Random Writing</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../math_main_page.html">Math</a><ul>
  <li><a href="information_theory_main_page.html">Information Theory</a><ul>
      <li>Previous: <a href="information_theory_main_page.html" title="previous chapter">Information Theory</a></li>
      <li>Next: <a href="crash_course_in_relative_entropy_i_mutual_information.html" title="next chapter">Crash Course in Relative Entropy I: Mutual Information</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Jeremy Mann.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.4.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/math/information_theory/crash_course_in_information_and_absolute_entropy.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>