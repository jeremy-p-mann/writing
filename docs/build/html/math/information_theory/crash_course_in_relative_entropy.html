
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Crash Course in Relative Entropy I: Mutual Information &#8212; Jeremy Mann&#39;s Writing 0.0.0 documentation</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Probability Theory" href="../probability_theory/probability_theory_main_page.html" />
    <link rel="prev" title="Crash Course in Information and Absolute Entropy" href="crash_course_in_information_and_absolute_entropy.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="crash-course-in-relative-entropy-i-mutual-information">
<h1>Crash Course in Relative Entropy I: Mutual Information<a class="headerlink" href="#crash-course-in-relative-entropy-i-mutual-information" title="Permalink to this headline">¶</a></h1>
<p>The set up is as follows:</p>
<p>A system <span class="math notranslate nohighlight">\(\Omega\)</span> along with two observables  <span class="math notranslate nohighlight">\(X,Y\)</span></p>
<div class="math notranslate nohighlight">
\[(X, Y): \Omega \longrightarrow \mathbb{R}^2\]</div>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>Pushing forward a probability distribution <span class="math notranslate nohighlight">\(\rho\)</span> gives:</p>
<div class="math notranslate nohighlight">
\[\rho_{X,Y} := (X, Y)_*(\rho) \in \mathrm{Prob}(\mathbb{R}^2)\]</div>
<p>This generates two distributions:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\rho_X = X_*(\rho) \\
\rho_Y = Y_*(\rho)\end{split}\]</div>
</div>
<p>An amazing feature of probability theory is that these two conditions are insufficient to reconstruct <span class="math notranslate nohighlight">\(\rho_{XY}\)</span>, unless you know that they are independent.</p>
<div class="section" id="entropy-dependency-between-variables">
<h2>Entropy &amp; Dependency Between Variables<a class="headerlink" href="#entropy-dependency-between-variables" title="Permalink to this headline">¶</a></h2>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is independent of <span class="math notranslate nohighlight">\(Y\)</span>, of the amount of information present in seeing both of them is the information contained in seeing <span class="math notranslate nohighlight">\(X\)</span> plus the information contained in seeing <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>At the level of expectation values, we obtain an expression:</p>
<div class="math notranslate nohighlight">
\[\mathcal{S}(\rho_{X \times Y})  = \mathcal{S}(\rho_X) + \mathcal{S}(\rho_Y)\]</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>We will adopt the standard abusive conflation of random variables and probability distributions, writing the formula below as:</p>
<div class="math notranslate nohighlight">
\[\mathcal{S}(X,Y)  = \mathcal{S}(X) + \mathcal{S}(Y)\]</div>
<p>We’d like to emphasize that the left side is the entropy of a joint distribution, while the right side is a sum of entropies of marginal distributions.</p>
</div>
<p>This additivity characterizes independence. If they are highly correlated, this is no longer the case, you’re “overestimating” the entropy:</p>
<div class="math notranslate nohighlight">
\[\mathcal{S}(X, Y) \leq \mathcal{S}(X) + \mathcal{S}(Y)\]</div>
<p>In other words, dependence between variables decreases entropy.</p>
<p>Therefore, the amount of this decrease can therefore be considered as a metric for how much <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> depend on each other.</p>
</div>
<div class="section" id="mutual-information">
<h2>Mutual Information<a class="headerlink" href="#mutual-information" title="Permalink to this headline">¶</a></h2>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<blockquote>
<div><p>The mutual information between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is defined as:</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[I(X, Y) = \mathcal{S}(X) + \mathcal{S}(Y) - \mathcal{S}(X,Y) \in \mathbb{R}\]</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If X and Y were independent the sum of the first two terms is the entropy of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. Therefore, the mutual information is how much you overestimated the average amount of information were you to assume they were independent.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As entropy is invariant under symmetries, mutual information is “symmetric”.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is another formulation, which is that the mutual information is just the entropy of <span class="math notranslate nohighlight">\(X\)</span> (“marginal”) minus the amount of information in <span class="math notranslate nohighlight">\(X\)</span> when <span class="math notranslate nohighlight">\(Y\)</span> is known (“conditional”). This formulation relies upon a common “forgetful” Bayesian contrustion:</p>
<div class="math notranslate nohighlight">
\[\mathrm{Prob}(X \times Y) \longrightarrow \mathrm{Maps}(X, \mathrm{Prob}(Y))\]</div>
<p>In other words, we assemble a joint distribution conditioned on observations of <span class="math notranslate nohighlight">\(X\)</span> into a map associating an event <span class="math notranslate nohighlight">\(x \in X\)</span> to the probability distribtion on <span class="math notranslate nohighlight">\(Y\)</span></p>
<div class="math notranslate nohighlight">
\[\rho_{X, Y}|_{x} = \rho_{X,Y}( - | x )\]</div>
<p>conventionally referred to as the conditional distribution.</p>
<p>Taking the entropy of this distribution conditional distribution gives a function whose expectation value is intimately related to mutual information:</p>
<div class="math notranslate nohighlight">
\[I(X, Y) = \mathcal{S}(X , Y) - \langle \mathcal{S}(\rho_{X, Y}|_{x}) \rangle_{\rho_X}\]</div>
</div>
</div>
<div class="section" id="example-noisy-channel">
<h2>Example: “Noisy Channel”<a class="headerlink" href="#example-noisy-channel" title="Permalink to this headline">¶</a></h2>
<p>In signal processing, mutual information quantifies to what extent one is system is the signal/receiver for another.</p>
<p>For example, <span class="math notranslate nohighlight">\(X\)</span> could be the presence of a certain type of stimuli, and <span class="math notranslate nohighlight">\(Y\)</span> could be the activation of a neuron some time later. In this case, a low amount of mutual information rejects the hypothesis that <span class="math notranslate nohighlight">\(X\)</span> can be considered as a stimuli for <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>Let’s imagine the stimuli <span class="math notranslate nohighlight">\(X\)</span> and the neuron <span class="math notranslate nohighlight">\(Y\)</span> can only take two states <span class="math notranslate nohighlight">\(x_0, x_1\)</span> and <span class="math notranslate nohighlight">\(y_0, y_1\)</span>. Moreover, let’s for simplicity, let’s assume that:</p>
<div class="math notranslate nohighlight">
\[P(y_0 | x_0) = P(y_1 | x_1) = p\]</div>
<p>As we have control over the stimuli, let’s assume that:</p>
<div class="math notranslate nohighlight">
\[P(x_0) = P(x_1) = \frac{1}{2}\]</div>
<p>(i.e. for simplicity assume it’s in a maximally entropic state). This is visualized as:</p>
<p># TODO picture of noisy channel.</p>
<p>Therefore, the mutual information is a function of a single variable <span class="math notranslate nohighlight">\(p\)</span>, which we can think of as the “strength of the signal”.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <span class="math notranslate nohighlight">\(p \neq 0, 1\)</span>, the channel is “noisy”.</p>
</div>
<p>Below is a graph of the mutual information:</p>
<p>#TODO add graph</p>
<p>Equivalently, symmetry arguments can be used to show that the mutual information is:</p>
<div class="math notranslate nohighlight">
\[1 - \mathcal{S}(p, 1-p)\]</div>
<p>This can also be obtained using the plug and chug method.</p>
<p>Where the second term is the entropy in <span class="math notranslate nohighlight">\(Y\)</span> (the response) when <span class="math notranslate nohighlight">\(X\)</span> (the stimuli) is known.</p>
<p>Note that now the mutual information is minimized when the channel is the noisiest. The mutual information is maximized when <span class="math notranslate nohighlight">\(p = 0\)</span> or <span class="math notranslate nohighlight">\(p = 1\)</span>, in which case it is 1.</p>
<p>More generally, there is some one-to-one correspondence between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{I(X, Y)}{\sqrt{\mathcal{S}(X)\mathcal{S}(Y)}} = 1\]</div>
<p>Indicating that they are in some sense “perfectly correlated”. Above, the one-to-one correspondence is <span class="math notranslate nohighlight">\(x_i \leftrightarrow y_i\)</span> when <span class="math notranslate nohighlight">\(p = 1\)</span></p>
<p>This quantity should be thought of as analogous to a Pearson correlation coefficient.</p>
</div>
<div class="section" id="example-correlated-gaussians">
<h2>Example: Correlated Gaussians<a class="headerlink" href="#example-correlated-gaussians" title="Permalink to this headline">¶</a></h2>
<p>Let’s say we’re  a 2D gaussian with features <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> with Pearson correlation coefficient <span class="math notranslate nohighlight">\(c_{XY}\)</span> and equal standard deviations. In this case, the mutual information takes on an elegant form:</p>
<div class="math notranslate nohighlight">
\[-\frac{1}{2} \log (1 - c_{XY}^2)\]</div>
<p>#TODO graph of gaussian</p>
<p>In other words:</p>
<ul class="simple">
<li><p>“perfectly correlated” Gaussian features have infinite mutual information!</p></li>
<li><p>Independent features has no mutual information.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This gives a conceptual, heuristic interpretation of Principal Component Analysis: you’re finding composite features which contain no information about each other.</p>
</div>
<p>Information is about comparing two distributions: the “real” distribution” and the distribution which assume the features are independent. In other words mutual information is a “relative” notion of entropy.</p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Jeremy Mann's Writing</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../learning/learning_main_page.html">Learning</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../math_main_page.html">Math</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../category_theory/category_theory_main_page.html">Category Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homotopy_theory/homotopy_theory_main_page.html">Homotopy Theory</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="information_theory_main_page.html">Information Theory</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="crash_course_in_information_and_absolute_entropy.html">Crash Course in Information and Absolute Entropy</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Crash Course in Relative Entropy I: Mutual Information</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/probability_theory_main_page.html">Probability Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../geometry/geometry_main_page.html">Geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/linear_algebra_main_page.html">Linear Algebra</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/physics_main_page.html">Physics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../longevity/longevity_main_page.html">Longevity</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../random/random_main_page.html">Random Writing</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../math_main_page.html">Math</a><ul>
  <li><a href="information_theory_main_page.html">Information Theory</a><ul>
      <li>Previous: <a href="crash_course_in_information_and_absolute_entropy.html" title="previous chapter">Crash Course in Information and Absolute Entropy</a></li>
      <li>Next: <a href="../probability_theory/probability_theory_main_page.html" title="next chapter">Probability Theory</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Jeremy Mann.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/math/information_theory/crash_course_in_relative_entropy.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>