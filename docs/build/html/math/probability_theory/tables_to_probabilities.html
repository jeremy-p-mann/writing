
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Tables to Probabilities &#8212; Jeremy Mann&#39;s Writing 0.0.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="tables-to-probabilities">
<h1>Tables to Probabilities<a class="headerlink" href="#tables-to-probabilities" title="Permalink to this headline">¶</a></h1>
<section id="slide-1">
<h2>Slide 1<a class="headerlink" href="#slide-1" title="Permalink to this headline">¶</a></h2>
<p>Hello. In this lecture, I’d like to describe a construction that goes from
things that can be represented in physical memory to a single mathematical
abstraction. The video represents “things that can be represented in physical
memory” as tables while probability distributions are the “single mathematical
abstraction”.</p>
</section>
<section id="slide-2">
<h2>Slide 2<a class="headerlink" href="#slide-2" title="Permalink to this headline">¶</a></h2>
<p>So I need to produce a probability distribution from the data of a table.</p>
<p>Probability distributions are precisely defined objects. To make my life easier,
I will begin by giving a precise definition of a table that’s convenient
for our discussion.</p>
<p>Next, I’ll introduce a general probabilistic construction.</p>
<p>We will then accomplish our goal by applying this construction to tables and
then describe the output.</p>
<p>To keep this video brief, I won’t give much motivation to this construction.
Suffice it to say that math gives us lots of tools to do things with
probability distributions.</p>
</section>
<section id="slide-3">
<h2>Slide 3<a class="headerlink" href="#slide-3" title="Permalink to this headline">¶</a></h2>
<p>To anchor our discussion, let’s review a specific table, called FooBar.</p>
<p>This table is indexed by a set, let’s call it I, with 4 elements, 0, 1, 2, 3.
There are two columns, ‘foo’ and ‘bar’. In other words the set of columns,
let’s call it J, has two elements.</p>
<p>Note that each row of FooBar is an element of a two dimensional space,
R J, the set of J-tuples of real numbers. Furthermore we can think of as the
set of maps from the set of columns, J to the real numbers.</p>
</section>
<section id="slide-4">
<h2>Slide 4<a class="headerlink" href="#slide-4" title="Permalink to this headline">¶</a></h2>
<p>So let’s see if we can extract a precise definition from this perspective on
tables. For simplicity, we’ll only consider every entry in our table is  float
float.</p>
<p>A table is the data of:</p>
<p>An set of indices, I which parametrizes the rows of the table.</p>
<p>An set of columns, J</p>
<p>Finally, we have the entries of the table, which we can think of as a map
from I to the set of J-tuples of real numbers.</p>
<p>In other words an association of, for every index i, J-many real number x i j.
In the lingua franca of machine learning, I’ll call the set of J-tuples of real
numbers feature space. From this perspective, a table is a map from a set
indexing some samples into a feature space.</p>
<p>Note that this definition is a “row-centric” definition.</p>
<p>That’s a bit abstract. Let’s recover some more familiar perspectives on tables.</p>
</section>
<section id="slide-5">
<h2>Slide 5<a class="headerlink" href="#slide-5" title="Permalink to this headline">¶</a></h2>
<p>Let’s say that there are N rows, and the index is 0 to N-1
Further, let’s assume there are m columns, let’s call them 0 to m-1.</p>
<p>In this case, the table is the data of for each whole number between 0 and
N-1, a vector of dimension m. I.e. N m-component vectors.</p>
<p>Alternatively, we can see this data as an n by m matrix of real numbers.</p>
<p>Although I should warn you that, although this</p>
<p>Finally, hopefully it’s clear that this data is the same as N samples
each of which has m features.</p>
<p>So yea, that’s the first part: a precise definition of a table.
And now for the probabilistic construction.</p>
</section>
<section id="slide-6">
<h2>Slide 6<a class="headerlink" href="#slide-6" title="Permalink to this headline">¶</a></h2>
<p>This construction takes two pieces of input:</p>
<p>First A map, let’s call it pi, between two sets: X_0 to X_1.</p>
<p>Second, a probability distribution on the source of pi, X_0, let’s call it rho.</p>
<p>The output of this construction is a probability distribution on the
target of pi, X_1, pi star rho. We call this distribution the pushforward of
rho along pi.</p>
<p>Of course, I still have to define this distribution. Recall that if you
know how to compute expectation values of real-valued function with respect
to a distribution, you know the distribution.</p>
<p>In our case, the pushforward of rho along pi is characterized by the following.</p>
<p>Note that, given a real-valued function on X_1, O, which I’ll call an observable,
note that we obtain an observable on X_0 by first applying pi and then
applying O. This new function is often to referred to as the pullback of
rho along pi.</p>
<p>As rho is a distribution on X_0 and the pullback of O along pi is an observable
on X_0, it.</p>
<p>Therefore, we can use this maneuvre to define the expectation value of
O with respect to the pushforward of rho along pi as the expectation of
the pullback of O along pi with respect to rho.</p>
<p>pushingforward as the adjoint of pulling back, aka precomposition.</p>
</section>
<section id="slide-7">
<h2>Slide 7<a class="headerlink" href="#slide-7" title="Permalink to this headline">¶</a></h2>
<p>So just to summarize, given a map of sets pi, we obtain a function
that sends a probability distribution, rho, on the souce of the map to
probability distribution, pi star rho, on the target of the map.</p>
<p>So we’re used to thinking of probability distributions as associating
numbers to events. Therefore, we should probably address the question, how
do we compute probabilities with respect to the pushforward of a distribution
in terms of the the map and the original distribution?</p>
</section>
<section id="slide-8">
<h2>Slide 8<a class="headerlink" href="#slide-8" title="Permalink to this headline">¶</a></h2>
<p>So our definition of is in terms observables. Therefore, if we want to say
something about events, aka measurable subsets of the event space, we should
think of events as a real-valued functions.</p>
<p>This is straight forward. Given a subset A, the function takes an element x to 1
if x is in A, and 0 otherwise. We’ll call this function chi sub A, the
characteristic function of A.</p>
<p>If you think a bit, you’ll realize that the probability of A is the expectation
value of chi sub A.</p>
<p>By definition, this is computed as the expectation value of the precomposition
of chi sub A with pi.</p>
<p>I’ll leave it as an exercise to show that this function is the characteristic
function of the pre-image of A with respect to pi.</p>
<p>Putting this all together, we see that the probability A with respect to pi
star A is the probability of all elements of X_0 which map to A under pi
with respect to rho.</p>
<p>Goal accomplished. Now, let’s go through some concrete examples.</p>
</section>
<section id="slide-9">
<h2>Slide 9<a class="headerlink" href="#slide-9" title="Permalink to this headline">¶</a></h2>
<p>For example, let’s say that pi projects a two dimensional space spanned by x
and y onto the x coordinate and a probability distribution on R two.</p>
<p>In this case, we can compute the probability density, at least heuristically,
as the following integral.</p>
<p>Classically, this pushforward goes by the name of the marginal distribution
of rho associated to X.</p>
</section>
<section id="slide-10">
<h2>Slide 10<a class="headerlink" href="#slide-10" title="Permalink to this headline">¶</a></h2>
<p>Let’s say that pi is the inclusion of a finite subset, let’s call it U,
of Euclidean space.</p>
<p>Let’s say we have the uniform distribution on X zero to X n.</p>
<p>In this case, the pushforward is a sum of dirac-delta distributions
supported on the subset.</p>
<p>So hopefully that’s enough examples. Let’s return to our original
goal of creating a probability distribution from a tale.</p>
</section>
<section id="slide-11">
<h2>Slide 11<a class="headerlink" href="#slide-11" title="Permalink to this headline">¶</a></h2>
<p>So let’s say we have a table. Furthermore, let’s assume that we have a
probability distribution on the set of indices of the table. Note that both
of these are finite data.</p>
<p>Now, since we defined a table to be a map, we can pushforward this
distribution along the table to obtain a probability distribution on the
space of features.</p>
<p>Therefore, all we need to do is produce a natural probability distribution on
the set of indices of the table. So I feel like taking a nonstandard route, and
use the pushforward construction to define this very special distribution:
the uniform distribution.</p>
</section>
<section id="slide-12">
<h2>Slide 12<a class="headerlink" href="#slide-12" title="Permalink to this headline">¶</a></h2>
<p>So let’s imagine that I have a permutation, sigma, of the set of indices and a
probability distribution, rho, on the set of indices.</p>
<p>As a permutation is, in particular, a map from I to I. Therefore I can a new
distribution defined as push rho along sigma. In other words, we can
think of permutations as acting on the set of probability distributions.</p>
<p>It’s a fun lemma to prove that there is a unique distribution which is
fixed by the action of every possible permutation of finite set.</p>
<p>This lemma in some sense defines what is commonly referred to as the uniform
distribution. Note that the lemma is essential in having a sensible definition,
as the lemma is not true for infinite sets.</p>
<p>If what I wrote confuses you, you can take the definition of the uniform
distribution as the distribution defined by the following formula.</p>
</section>
<section id="slide-13">
<h2>Slide 13<a class="headerlink" href="#slide-13" title="Permalink to this headline">¶</a></h2>
<p>So now I can use the table to pushforward the uniform distribution, obtaining
a distribution on the space of features. This distribution is sometimes
referred to as the “emprirical distribution”, as it is constructed directly
from the data.</p>
<p>I’ll leave it as an exercise to show that the probability of an event
is the number of samples in which that event occurred divided by the total
number of samples.</p>
<p>Moreover, the expectation value of an observable is the average value of that
observable over all samples. This formula is usually how I recognize the
emprirical distribution in more standard statistics literature.</p>
</section>
<section id="slide-14">
<h2>Slide 14<a class="headerlink" href="#slide-14" title="Permalink to this headline">¶</a></h2>
<p>So let’s the example of the FooBar table. I’ll leave it as an exercise to
show that it’s the sum of three delta distributions corresponding to the
quote “normalized value counts” of the table.</p>
<p>Note that we are losing information in passing to the emprirical distribution.
We no longer have access to the index, ordering or primary key of the table.</p>
<p>This is reflected in the fact that this construction is invariant under
permutation of the rows of the table.</p>
<p>For example, there’s no analogue of the operation of join along a primary
key of the table for the empirical distribution. In practice, this construction
is applied to wrangles/joins of a bunch of tables in a relational database.</p>
</section>
<section id="slide-15">
<h2>Slide 15<a class="headerlink" href="#slide-15" title="Permalink to this headline">¶</a></h2>
<p>In summary, we’ve defined a construction from data, in the form of tables,
to math, in the form probabilities distribution by pushing forward the uniform
distribution on the set of indices to feature space.</p>
</section>
<section id="slide-16">
<h2>Slide 16<a class="headerlink" href="#slide-16" title="Permalink to this headline">¶</a></h2>
<p>So I’d like give you some idea of how this is used in practice, by discussing
a standard approach to using data to construct a model.</p>
<p>First, let’s imagine we have a table and parametric model that associates a parameter
theta a probability distribution rho theta on the relevant feature space.</p>
<p>Then we can combine the use the relative entropy functional, aka the KL
divergence and the empirical distribution associated to the table to construct
a loss function on the parameter space.</p>
<p>Although this function has many definitions, I’d like to comment that it admits a
definition that’s purely in terms of A/B testing. For now, I’ll just note
that it can be computed via the following formula.</p>
</section>
<section id="slide-17">
<h2>Slide 17<a class="headerlink" href="#slide-17" title="Permalink to this headline">¶</a></h2>
<p>This loss function forms the basis of maximum likelihood theory, which
defines the model as the minimum of this loss function.</p>
<p>Imagining we are in a sufficiently smooth setting, this is the vanishing locus
of</p>
<p>As an aside, I’d like to note that we should think not of the quote “gradient”
of</p>
</section>
<section id="slide-18">
<h2>Slide 18<a class="headerlink" href="#slide-18" title="Permalink to this headline">¶</a></h2>
<p>As a final note, we can wonder, what if we used something other than the
uniform distribution on the set of indices?</p>
<p>What if we gave more weight to some of the indices, maybe in a way that depends
on something not present in feature space.</p>
<p>At least that’s how I’d like to think of inverse probability weighting.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Jeremy Mann's Writing</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../learning/learning_main_page.html">Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math_main_page.html">Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/physics_main_page.html">Physics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../longevity/longevity_main_page.html">Longevity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/software_main_page.html">Software Stuff</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stream/stream_main_page.html">Stream Main Page</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../to_do.html">To Do List</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../random/random_main_page.html">Random Writing</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Jeremy Mann.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/math/probability_theory/tables_to_probabilities.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>