Fundamental Ingredients of Statistical Learning
===============================================

The goal of these notes are to establish a basic general framework to discuss learning algorithms.

We begin with a brief discussion of 

Sample Data
-----------

Usually, sample data is thought of a bunch of tuples :math:`(x_i, y_i)` for all :math:`i=0, \ldots, N`.

In other words, it's a map:

.. math:: 

   I &\overset{S}\longrightarrow \mathcal{X} \times \mathcal{Y} \\
   i &\longmapsto S_i =: (x_i, y_i)

.. admonition:: Definition
   
    Let :math:`\rho_I^\mathrm{unif}` denote the uniform distribution on the set :math:`I`. Viewing the sample as a map allows us to efficiently generate a distribution on :math:`\mathcal{X} \times \mathcal{Y}`:

   .. math:: 

      \rho_S = S_* \rho_I 

   which we shall refer to as the *empirical distribution* associated to the sample :math:`S`. 

.. note::

   In classical information theory, the empirical distribution is refered to as the *type* of the sample. 

.. admonition:: Motivation
   
   In practice, the ordering of a sample has any meaning in terms of the problem at hand. More concretely, no learning algorithm should depend on , as any such dependence would introduce superfluous variance to the model. More importantly, this variance is not offest by any reduction in bias. 

   Therefore, if we wanted to count the number of of all possible samples (i.e. the volume of the moduli space of samples), we must not overcount the number of samples by individually counting samples differing by a permutation. In other words, every meaningful feature  

   One benefit of the notion of the empirical distribution is that it naturally avoids the quotient process. In other words, it's a convenient representation of the moduli space of samples.

.. note::

   The conciseness of this construction highlights the utility of thinking about a sample as a map. Moreover, different choices for the distribution on :math:`I` yield different "class weights" for the learning problem, and are the basis for boosting algorithms.

For completeness, we introduce some standard, informal terms

.. admonition:: Definition

   We call :math:`\mathcal{X}` the *space of features*, and :math:`mathcal{Y}` the *space of labels*

Models and Hypotheses
---------------------

.. admonition:: Definition
   
   *The* Hypothesis Set:

- discriminative learning problem
- generative learning problem

.. admonition:: Definition

   *A* Discriminative Model Space

   We call :math:`\tilde{f}` the  model's *underlying hypothesis*. 

.. note::

   This map need not be an embedding. For example, when the model space is the space of weights for a neural network with at least one hidden layer due to weight space symmetries. 

.. admonition:: Example   

   Logistic Regression modelsspace/hypothesis space
   
Loss Functions
--------------

A loss function is a pairing between data and models. In other words, it's a map:

.. admonition:: Definition
   A loss function for a learning algorithm 
   .. math:: 
   
      \mathrm{Prob}(\mathcal{X}\times\mathcal{Y})\times \mathcal{M} &\overset{\mathscr{L}}\longrightarrow \mathbb{R} \\
      (\rho, f) &\longmapsto \mathscr{L}_\rho (f)

.. note::

   One might want to say that a loss function is a function of the hypothesis space. This would exlude the possibility of additional regularization terms to the loss function.

.. admonition:: Example
   
   Information theory gives powerful loss functions in the form of relative entropy: 




