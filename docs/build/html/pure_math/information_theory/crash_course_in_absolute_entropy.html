
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Crash Course in Absolute Entropy &#8212; Jeremy Mann&#39;s Writing 0.0.0 documentation</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Probability Theory" href="../probability_theory/probability_theory_main_page.html" />
    <link rel="prev" title="Information Theory" href="information_theory_main_page.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="crash-course-in-absolute-entropy">
<h1>Crash Course in Absolute Entropy<a class="headerlink" href="#crash-course-in-absolute-entropy" title="Permalink to this headline">¶</a></h1>
<p>Although information theory is omnipresent in statistical inference/learning, most introductions concentrate on the coding or statistical physics interpretation of information. The goal of these notes is to demonstrate, through theory and example, the power of information theory in machine learning and statistics.</p>
<p>These notes are highly experimental. We ask the reader to forgive the cold, terse tone of this exposition. Any comments on confusions or mistakes are greatly appreciated.</p>
<div class="section" id="information">
<h2>Information<a class="headerlink" href="#information" title="Permalink to this headline">¶</a></h2>
<p>Gibbs and Shannon taught us that the likelihood of an event (according to a hypothesis) is encapsulated in the amount of useful information (according to a hypothesis) present it’s observation. Heuristically, it’s “the information in an event scales with the amount of surprise upon it’s observation.”</p>
<p>Let’s imagine you’re computing information in bits, and <span class="math notranslate nohighlight">\(\rho\)</span> was the uniform distribution on binary sequences of length <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p>In this case, the information contained in a single string <span class="math notranslate nohighlight">\(S\)</span> coincides with it’s length. This happens to the be:</p>
<div class="math notranslate nohighlight">
\[\mathcal{I}_\rho(S) = \log_2((.5^n)^{-1}) = -\log_2{(.5^n)} = n\]</div>
<p>For example, “010” contains 3 bits of information</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>Given a fixed probability distribution, with density <span class="math notranslate nohighlight">\(\rho\)</span>, with sample space <span class="math notranslate nohighlight">\(\mathscr{X}\)</span>. For any <span class="math notranslate nohighlight">\(x \in \mathscr{X}\)</span>, the <strong>information</strong> in x is:</p>
<div class="math notranslate nohighlight">
\[\mathcal{I}_{\rho}(x) = \log\Bigl(\frac{1}{p(x)}\Bigl) = - \log(p(x))\]</div>
</div>
<p>The notion of information depends explicitly on the probability distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While an event with probability 1 has no information, an event with probability 0 has an infinite amount of information.</p>
</div>
<p>Throughout, we will be abusive with which base we take our logarithm with respect to, according to convenience. The choice of a base can be thought of choice of units, and can be recovered by examining the information of obtaining a heads in a fair coin toss.</p>
<p>Below is a plot of information of an event as a function of it’s likelihood:</p>
<img alt="../../_images/probability_vs_information.png" src="../../_images/probability_vs_information.png" />
<p>Heuristically, information is dual to likelihood/probability:</p>
<ul class="simple">
<li><dl class="simple">
<dt>High likelihoood events have litle (useful!) information.</dt><dd><ul>
<li><p>Knowing it’s not going to rain next saturday has little useful information if you live in a desert</p></li>
<li><p><span class="math notranslate nohighlight">\(2 + 2 = 4\)</span> (which we’re certain is true) has zero information</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Low likelihood events have</dt><dd><ul>
<li><p>Knowing it’s going to be sunny next saturday has a lot of useful information if you live in Seattle</p></li>
<li><p><span class="math notranslate nohighlight">\(2 + 2 \neq 4\)</span> has an infinite amount of information</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="entropy">
<h2>Entropy<a class="headerlink" href="#entropy" title="Permalink to this headline">¶</a></h2>
<p>Information is a pairing between events and probability distributions. More formally, it’s a function:</p>
<div class="math notranslate nohighlight">
\[\mathrm{Prob}(\mathcal{X}) \times \mathcal{X} \longrightarrow \mathbb{R}\]</div>
<p>In other words, it gives a random variable for each probability distribution <span class="math notranslate nohighlight">\(\rho\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathcal{X} \overset{\mathcal{I}_{\rho}}\longrightarrow \mathbb{R}\]</div>
<p>whose expectation value provides a numerical invariant of <span class="math notranslate nohighlight">\(\rho\)</span>, namely it’s entropy.</p>
<p>We briefly pause to introduce some</p>
<div class="admonition-notation admonition">
<p class="admonition-title">Notation</p>
<p>Given a random variable <span class="math notranslate nohighlight">\(X\)</span> and a probability distribution <span class="math notranslate nohighlight">\(\rho\)</span>, we let</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_\rho[X] = \langle X \rangle_\rho\]</div>
<p>denote the expectation value of <span class="math notranslate nohighlight">\(X\)</span> with respect to the distribution <span class="math notranslate nohighlight">\(\rho\)</span></p>
</div>
<p>We can now state the</p>
</div>
<div class="section" id="definition-of-entropy">
<h2>Definition of Entropy<a class="headerlink" href="#definition-of-entropy" title="Permalink to this headline">¶</a></h2>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>The entropy of <span class="math notranslate nohighlight">\(\rho\)</span> is just the expected amount of information <span class="math notranslate nohighlight">\(\rho\)</span> contains:</p>
<div class="math notranslate nohighlight">
\[\mathcal{S}(\rho) =  \langle  \mathcal{I}_{\rho} \rangle_{\rho}\]</div>
</div>
<p>where,  denotes the expectation value of the function (i.e. ‘random variable’) <span class="math notranslate nohighlight">\(X\)</span> with respect to the distribution <span class="math notranslate nohighlight">\(\rho\)</span>.</p>
<p>Below is the a graph of a the entropy of biased coin toss as function of the probability of heads:</p>
<img alt="../../_images/entropy_of_biased_coin.png" src="../../_images/entropy_of_biased_coin.png" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The entropy is maximized when the coin is fair! Moreover, entropy is minimized when the coin has no randomness!
This is the first indication that we can think of entropy as quantifying the amount of uncertainty in a system.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>An essential feature of this definitions is that <span class="math notranslate nohighlight">\(x\)</span> goes to 0 much faster than $log(x)$. In other words,</p>
<div class="math notranslate nohighlight">
\[\lim_{p \rightarrow 0^+} \bigl(- p \log(p) \bigl) = 0\]</div>
<p>This ensures that impossible events do not cause the entropy to be infinite.</p>
</div>
</div>
<div class="section" id="entropy-of-a-normal-distribution">
<h2>Entropy of a Normal Distribution<a class="headerlink" href="#entropy-of-a-normal-distribution" title="Permalink to this headline">¶</a></h2>
<p>For a(n univariate) normal distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma)\)</span>, the amount of information (in “natural units”) is easy to compute:</p>
<div class="math notranslate nohighlight">
\[\mathcal{I}_{\mathcal{N}(\mu, \sigma)}(x) = \frac{1}{2} \Bigl(\frac{x - \mu}{\sigma}\Bigl)^2 - \log( \sigma\sqrt{2 \pi})\]</div>
<p>The entropy of a univariate normal distribution (in bits) is gorgeous:</p>
<div class="math notranslate nohighlight">
\[\mathcal{S}\bigl(\mathcal{N}(\mu, \sigma) \bigl)= \log ( \sigma) + \log ( \sqrt{2e\pi})\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We see further evidence for the heuristic that entropy quantifies the randomness of the distribution, as it increase monotonically with the standard deviation.</p>
<p>Moreover, the entropy does not depend on the expectation value of the dist.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In general, entropy is translation/permutation invariant. Fortunately, it is not dialation invariant.</p>
</div>
<p>The formula for a multivariate Gaussian is analagous but more complex. What’s most interesting is a manifestation of the curse of dimensionality:</p>
<div class="math notranslate nohighlight">
\[\mathcal{S}\bigl(\mathcal{N}(0, \mathbb{1}_n) \Bigl)= n \cdot \mathcal{S}\big(\mathcal{N}(0, 1) \bigl)\]</div>
<p>So that the entropy of a unit normal scales linearly with the dimension.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A general fact is that normal distributions maximize the entropy amongst all distributions with a fixed mean and variance.</p>
<p>More generally, all exponential families (e.g. Boltzman distributions, exponential, multinomial, …) arise by a similar “Maximum Entropy Principle.”</p>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Jeremy Mann's Writing</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../learning/learning_main_page.html">Learning</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../pure_math_main_page.html">Pure Math</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../category_theory/category_theory_main_page.html">Category Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homotopy_theory/homotopy_theory_main_page.html">Homotopy Theory</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="information_theory_main_page.html">Information Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/probability_theory_main_page.html">Probability Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../geometry/geometry_main_page.html">Geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/linear_algebra_main_page.html">Linear Algebra</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/physics_main_page.html">Physics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../longevity/longevity_main_page.html">Longevity</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../random/random_main_page.html">Random Writing</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../pure_math_main_page.html">Pure Math</a><ul>
  <li><a href="information_theory_main_page.html">Information Theory</a><ul>
      <li>Previous: <a href="information_theory_main_page.html" title="previous chapter">Information Theory</a></li>
      <li>Next: <a href="../probability_theory/probability_theory_main_page.html" title="next chapter">Probability Theory</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Jeremy Mann.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.1.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/pure_math/information_theory/crash_course_in_absolute_entropy.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>