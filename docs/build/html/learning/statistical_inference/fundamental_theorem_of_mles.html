
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>The Statement of Fundamental Theorem of MLEs &#8212; Jeremy Mann&#39;s Writing 0.0.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Asymptotic Computations of MLEs" href="mle_computations.html" />
    <link rel="prev" title="An Introduction to MLE’s" href="intro_to_mles.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="the-statement-of-fundamental-theorem-of-mles">
<h1>The Statement of Fundamental Theorem of MLEs<a class="headerlink" href="#the-statement-of-fundamental-theorem-of-mles" title="Permalink to this headline">¶</a></h1>
<p>Loosely, the Fundamental Theorem of Maximum Likelihood Estimators states:</p>
<div class="admonition-theorem admonition">
<p class="admonition-title">Theorem</p>
<p>Maximum likelihood estimators are asymptotically normal.</p>
</div>
<p>Making precise sense of this requires considerable work.</p>
<section id="recollections">
<h2>Recollections<a class="headerlink" href="#recollections" title="Permalink to this headline">¶</a></h2>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>The relative entropy:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\mathcal{D}(\rho_A || \rho_B) &amp;= \langle I_{\rho_B} - I_{\rho_A} \rangle_{\rho_A} \\
&amp;= \langle I_{\rho_B} \rangle_{\rho_A} - \mathcal{S}(\rho_A)
\end{align*}\end{split}\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(I_\rho\)</span> is the information associated to the distibution
<span class="math notranslate nohighlight">\(\rho_A\)</span></p>
</div>
<p>We begin with a brief review.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Recall that, given a parametric family:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\Theta &amp;\overset{\theta}\longrightarrow \mathrm{Prob}(\Omega) \\
\theta &amp;\longmapsto \rho_\theta
\end{align*}\end{split}\]</div>
</div></blockquote>
<p>maximum likelihood estimation provides a map:</p>
<blockquote>
<div><blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\mathfrak{D}(\Omega) &amp;\overset{\mathrm{MLE}_\Theta}\longrightarrow
\mathrm{Prob}(\Omega) \\
\rho_X &amp;\longmapsto \hat{\rho}_\theta(X)
= \mathrm{MLE_\Theta}(\rho_x) := \
\mathrm{argmin}_\theta \mathfrak{D}(\rho_X || \rho_\theta
\end{align*}\end{split}\]</div>
</div></blockquote>
<p>When the data is drawn from a probability distribution,
<span class="math notranslate nohighlight">\(\rho \in \mathrm{Prob}(\Omega)\)</span>, the MLE map gives a probability
distribution on the space of probability distributions:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathrm{MLE}_*(\rho_\theta) :=
\hat{\rho_\theta} \in \mathrm{Prob}(\Omega)\]</div>
</div></blockquote>
</div></blockquote>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Although <span class="math notranslate nohighlight">\(\hat{\rho}\)</span> is “random” (in the sense that it is a
a probability distribution) it is not a “random variable”. This subtle,
technical point is meant to emphasize the intrinsic nature of MLEs.</p>
<p>However, a choice of coordinates allows us to consider <span class="math notranslate nohighlight">\(\hat{\rho}\)</span>
a random variable.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Stein’s Lemma interprets the function MLEs are trying to minimize
interpretation in terms of hypothesis testing.</p>
</div>
</section>
<section id="geometric-preliminaries">
<h2>Geometric Preliminaries<a class="headerlink" href="#geometric-preliminaries" title="Permalink to this headline">¶</a></h2>
<p>Given a smooth function on a manifold e.g. (<span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>):</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[M \overset{f}\longrightarrow \mathbb{R}\]</div>
</div></blockquote>
<p>along with a “dummy” metric, <span class="math notranslate nohighlight">\(g\)</span>, we can construct a symmetric quadratic
form, the Hessian of <span class="math notranslate nohighlight">\(f\)</span>:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathrm{Hess}_g(f) \in \mathrm{Sym}^2(\mathrm{T}^*M)\]</div>
</div></blockquote>
<p>which can be computed as second derivatives in coordinates.</p>
<p>In general, this form depends on the dummy metric. However, if:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathrm{d}f|_p = 0\]</div>
</div></blockquote>
<p>then</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathrm{Hess}_g(f)|_p \in \mathrm{Sym}^2(\mathrm{T}^*_p M)\]</div>
</div></blockquote>
<p>is independent of the dummy metric independent of the dummy metric.</p>
<p>Moreover, if <span class="math notranslate nohighlight">\(f\)</span> is convex, <span class="math notranslate nohighlight">\(\mathrm{Hess}_g(f)|_p\)</span> is a
positive definite symmetric quadratic form on <span class="math notranslate nohighlight">\(\mathrm{T}_p M\)</span>.</p>
<p>Given coordinates <span class="math notranslate nohighlight">\(\varphi\)</span>, this can be computed as:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[(\partial_i \partial_j \varphi^*f)(p) \in \mathbb{R}\]</div>
</div></blockquote>
<p>Moreover, one can explicitly compute the covariance matrix using the following
construction:</p>
</section>
<section id="back-to-statistics">
<h2>Back to Statistics<a class="headerlink" href="#back-to-statistics" title="Permalink to this headline">¶</a></h2>
<p>In the setting of MLE, the dictionary is:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
 \Theta &amp;:= M \\
 f(\rho) &amp;:= \mathcal{D}(\rho_\theta || \rho) \in \mathrm{C}^\infty(\Theta)
\end{align*}\end{split}\]</div>
</div></blockquote>
<p>As we vary <span class="math notranslate nohighlight">\(\theta\)</span>, we obtain an element of:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[f_\theta \in \Theta \times \mathrm{C}^\infty(\Theta)\]</div>
</div></blockquote>
<p>In other words, the function which MLEs are trying to minimize gives
a positive definite quadratic form on the tangent space of
<span class="math notranslate nohighlight">\(\hat{\rho}_\theta\)</span>.</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>The Fisher information metric is defined as:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathbb{I}_\Theta = \left.
\mathrm{Hess} \bigl( \mathcal{D}(\rho_\theta || \rho) \bigl)
\right\vert_{\rho = \rho_\theta}
\in \mathrm{Sym}^2(\mathrm{T}^*\Theta)\]</div>
</div></blockquote>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As this metric is positive definite and symmetric, it defines a Riemannian
metric on <span class="math notranslate nohighlight">\(\Theta\)</span>, conventionally referred to as the Fisher-Rao
metric</p>
</div>
<div class="admonition-theorem admonition">
<p class="admonition-title">Theorem</p>
<p>When <span class="math notranslate nohighlight">\(g \in \mathrm{Im}(\theta)\)</span>, in the limit of
<span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\hat{\rho}_\theta \sim
\mathcal{N} \bigl(
g, (n \cdot \mathbb{I}|_{\hat{\rho}_\theta)})^{-1}
\bigl)\]</div>
</div></blockquote>
<p>where:</p>
</div>
<div class="admonition-lemma admonition">
<p class="admonition-title">Lemma</p>
<p>When <span class="math notranslate nohighlight">\(g \in \mathrm{Im}(\theta)\)</span> (i.e. the model is correctly
“specified”, MLE’s are consistent.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As will be seen in the next sections, the relative entropy is a
generalization of the effect size between two normal distibutions with
identical standard deviations.</p>
</div>
<div class="admonition-theorem admonition">
<p class="admonition-title">Theorem</p>
<p>Maximum likelihood estimation is positive definite and convex.</p>
</div>
<div class="admonition-corrollary admonition">
<p class="admonition-title">Corrollary</p>
<p>The central limit theorem?</p>
</div>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Jeremy Mann's Writing</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../learning_main_page.html">Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../machine_learning/machine_learning_main_page.html">Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../statistical_learning/statistical_learning_main_page.html">Statistical Learning</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="statistical_inference_main_page.html">Statistical Inference</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="permutation_test.html">The Permutation Test</a></li>
<li class="toctree-l3"><a class="reference internal" href="survival_analysis.html">Elements of Survival Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="mle.html">The Theory Maximum Likelihood Estimators</a></li>
<li class="toctree-l3"><a class="reference internal" href="intro_to_mles.html">An Introduction to MLE’s</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">The Statement of Fundamental Theorem of MLEs</a></li>
<li class="toctree-l3"><a class="reference internal" href="mle_computations.html">Asymptotic Computations of MLEs</a></li>
<li class="toctree-l3"><a class="reference internal" href="relative_entropy_of_gaussians.html">Relative Entropy of Gaussian Distributions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data_science/data_science_main_page.html">Data Science</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../math/math_main_page.html">Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/physics_main_page.html">Physics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../longevity/longevity_main_page.html">Longevity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/software_main_page.html">Software Stuff</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stream/stream_main_page.html">Stream Main Page</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../to_do.html">To Do List</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../random/random_main_page.html">Random Writing</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../learning_main_page.html">Learning</a><ul>
  <li><a href="statistical_inference_main_page.html">Statistical Inference</a><ul>
      <li>Previous: <a href="intro_to_mles.html" title="previous chapter">An Introduction to MLE’s</a></li>
      <li>Next: <a href="mle_computations.html" title="next chapter">Asymptotic Computations of MLEs</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Jeremy Mann.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/learning/statistical_inference/fundamental_theorem_of_mles.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>