
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fundamental Ingredients of Statistical Learning &#8212; Jeremy Mann&#39;s Writing 0.0.0 documentation</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Notions and Interrelationships" href="notions_of_error_in_machine_learning.html" />
    <link rel="prev" title="Statistical Learning" href="statistical_learning_main_page.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="fundamental-ingredients-of-statistical-learning">
<h1>Fundamental Ingredients of Statistical Learning<a class="headerlink" href="#fundamental-ingredients-of-statistical-learning" title="Permalink to this headline">¶</a></h1>
<p>The goal of these notes are to establish a basic general framework to discuss learning algorithms.</p>
<p>We begin with a brief discussion of</p>
<div class="section" id="sample-data">
<h2>Sample Data<a class="headerlink" href="#sample-data" title="Permalink to this headline">¶</a></h2>
<p>Usually, sample data is thought of a bunch of tuples <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> for all <span class="math notranslate nohighlight">\(i=0, \ldots, N\)</span>.</p>
<p>In other words, it’s a map:</p>
<div class="math notranslate nohighlight">
\[\begin{split}I &amp;\overset{S}\longrightarrow \mathcal{X} \times \mathcal{Y} \\
i &amp;\longmapsto S_i =: (x_i, y_i)\end{split}\]</div>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>Let <span class="math notranslate nohighlight">\(\rho_I^\mathrm{unif}\)</span> denote the uniform distribution on the set <span class="math notranslate nohighlight">\(I\)</span>. Pushing this distribution forward along <span class="math notranslate nohighlight">\(S\)</span> gives a distribution on <span class="math notranslate nohighlight">\(\mathcal{X} \times \mathcal{Y}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\rho_S = S_* \bigl(\rho_I^\mathrm{unif}\bigl)\]</div>
<p>we shall refer to as the <em>empirical distribution</em> associated to the sample <span class="math notranslate nohighlight">\(S\)</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We leave it as an exercise to show that the probability of <span class="math notranslate nohighlight">\((x, y)\in \mathcal{X} \times \mathcal{Y}\)</span> is the number of times <span class="math notranslate nohighlight">\((x, y)\in\mathcal{X} \times \mathcal{Y}\)</span> was observed in the sample divided by the number of samples.</p>
<p>In particular, the empirical distribution is a finite weighted sum of dirac distributions. In other words, it has finite support.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Moreover, different choices for the distribution on <span class="math notranslate nohighlight">\(I\)</span> yield different “class weights” for the learning problem, and are the basis for boosting algorithms.</p>
</div>
<p>For completeness, we introduce some standard, informal terms</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>We call <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> the <em>space of features</em>, and <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> the <em>space of labels</em></p>
</div>
</div>
<div class="section" id="models-and-hypotheses">
<h2>Models and Hypotheses<a class="headerlink" href="#models-and-hypotheses" title="Permalink to this headline">¶</a></h2>
<p>Models in statistical learning come in two flavors: discriminative and generative.</p>
<p>A discriminative model is an assignment, for every feature, a probability distribution on the space of labels. A generative model on the other hand, assigns probability ofns probability of</p>
<p>The two definitions below make these heuristics precise.</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>A <em>discriminative hypothesis</em> is a map:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X &amp;\overset{f}\longrightarrow \mathrm{Prob}(Y) \\
x &amp;\longmapsto f_x(y) = &quot;f( - | x)&quot;\end{split}\]</div>
</div>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>A <em>generative hypothesis</em> is a probability distribution on <span class="math notranslate nohighlight">\(X \times Y\)</span>:</p>
<div class="math notranslate nohighlight">
\[f \in \mathrm{Prob}(X \times Y)\]</div>
</div>
<p>There are standard ways of going between these two pictures. These usually involve a combination of Bayesian and Maximum Likelihood techniques.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input from Bayesian theory is the equivalence:</p>
<div class="math notranslate nohighlight">
\[\mathrm{Prob}(X \times Y) \simeq \mathrm{Prob}(X) \times \mathrm{Maps}(X, \mathrm{Prob}(Y))\]</div>
<p>Hopefully, reading this equivalence from left to right makes it clear that a generative model is equivalent to specifying a discriminative model and a distribution on X.</p>
<p>Going from generative into discriminative amounts to forgetting the marginal distribution on X:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathrm{Prob}(X \times Y) \simeq \mathrm{Prob}(X) \times \mathrm{Maps}(X, \mathrm{Prob}(Y)) &amp;\longrightarrow \mathrm{Maps}(X, \mathrm{Prob}(Y)) \\
\rho &amp;\longmapsto \bigl(x \mapsto \rho(- | x)\bigl)\end{split}\]</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In particular, distributions which maximize entropy subject to predetermined constraints give dictionaries between generative and discriminative models. The standard example comes from uniform distributions on X.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>At times we may abusively neglect to say whether we are in a discriminative or generative setting. Hopefully the preceding notes give an idea of how the reader can infer the appropriate context.</p>
</div>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>We will denote the space of hypotheses as H.</p>
<p>More preciesely, in the discriminative setting:</p>
<div class="math notranslate nohighlight">
\[H := \mathrm{Maps}(X, \mathrm{Prob}(Y))\]</div>
<p>while in the generative setting:</p>
<div class="math notranslate nohighlight">
\[H := \mathrm{Prob}(X \times Y)\]</div>
</div>
<p>It’s important to differentiate hypotheses and models thereof.</p>
<p>Obviously, every model should give a hypothesis. However, it’s sometimes helpful to either restrict (impose constraints) or enlarge (factor through a pushfoward) the models we are optimizing over.</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>A model space is the data of a <em>parameter space</em> <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> and a map:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathcal{M} &amp;\longrightarrow H \\
f &amp;\longmapsto \tilde{f}\end{split}\]</div>
<p>We call <span class="math notranslate nohighlight">\(\tilde{f}\)</span> the  model’s <em>underlying hypothesis</em>. M is also referred to as the <em>parameter space</em>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This map need not be surjective. When this is the case, we call the model parametric. I will not use the language of parametric vs nonparametric, as its meaning is either ill-defined or utterly trivial.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This map need not be an embedding. For example, when the model space is the space of weights for a neural network with at least one hidden layer due to weight space symmetries.</p>
</div>
</div>
<div class="section" id="loss-functions">
<h2>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<p>A loss function is a pairing between data and models. In other words, it’s a map:</p>
<div class="admonition-definition-a-loss-function-for-a-learning-algorithm-math admonition">
<p class="admonition-title">Definition
A loss function for a learning algorithm
.. math::</p>
<blockquote>
<div><p>mathrm{Prob}(mathcal{X}timesmathcal{Y})times mathcal{M} &amp;overset{mathscr{L}}longrightarrow mathbb{R} \
(rho, f) &amp;longmapsto mathscr{L}_rho (f)</p>
</div></blockquote>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One might want to say that a loss function is a function of the hypothesis space. This would exlude the possibility of additional regularization terms to the loss function.</p>
</div>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>Information theory gives powerful loss functions in the form of relative entropy:</p>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Jeremy Mann's Writing</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../learning_main_page.html">Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../machine_learning/machine_learning_main_page.html">Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../statistical_inference/statistical_inference_main_page.html">Statistical Inference</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="statistical_learning_main_page.html">Statistical Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_science/data_science_main_page.html">Data Science</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../math/math_main_page.html">Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/physics_main_page.html">Physics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../longevity/longevity_main_page.html">Longevity</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../random/random_main_page.html">Random Writing</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../learning_main_page.html">Learning</a><ul>
  <li><a href="statistical_learning_main_page.html">Statistical Learning</a><ul>
      <li>Previous: <a href="statistical_learning_main_page.html" title="previous chapter">Statistical Learning</a></li>
      <li>Next: <a href="notions_of_error_in_machine_learning.html" title="next chapter">Notions and Interrelationships</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Jeremy Mann.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/learning/statistical_learning/fundamental_ingredients_of_statistical_learning.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>