
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fundamental Ingredients of Statistical Learning &#8212; Jeremy Mann&#39;s Writing 0.0.0 documentation</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Notions and Interrelationships" href="notions_of_error_in_machine_learning.html" />
    <link rel="prev" title="Statistical Learning" href="statistical_learning_main_page.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="fundamental-ingredients-of-statistical-learning">
<h1>Fundamental Ingredients of Statistical Learning<a class="headerlink" href="#fundamental-ingredients-of-statistical-learning" title="Permalink to this headline">¶</a></h1>
<p>The goal of these notes are to establish a basic general framework to discuss learning algorithms.</p>
<p>We begin with a brief discussion of</p>
<div class="section" id="sample-data">
<h2>Sample Data<a class="headerlink" href="#sample-data" title="Permalink to this headline">¶</a></h2>
<p>Usually, sample data is thought of a bunch of tuples <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> for all <span class="math notranslate nohighlight">\(i=0, \ldots, N\)</span>.</p>
<p>In other words, it’s a map:</p>
<div class="math notranslate nohighlight">
\[\begin{split}I &amp;\overset{S}\longrightarrow \mathcal{X} \times \mathcal{Y} \\
i &amp;\longmapsto S_i =: (x_i, y_i)\end{split}\]</div>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<blockquote>
<div><p>Let <span class="math notranslate nohighlight">\(\rho_I^\mathrm{unif}\)</span> denote the uniform distribution on the set <span class="math notranslate nohighlight">\(I\)</span>. Viewing the sample as a map allows us to efficiently generate a distribution on <span class="math notranslate nohighlight">\(\mathcal{X} \times \mathcal{Y}\)</span>:</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\rho_S = S_* \rho_I\]</div>
<p>which we shall refer to as the <em>empirical distribution</em> associated to the sample <span class="math notranslate nohighlight">\(S\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In classical information theory, the empirical distribution is refered to as the <em>type</em> of the sample.</p>
</div>
<div class="admonition-motivation admonition">
<p class="admonition-title">Motivation</p>
<p>In practice, the ordering of a sample has any meaning in terms of the problem at hand. More concretely, no learning algorithm should depend on , as any such dependence would introduce superfluous variance to the model. More importantly, this variance is not offest by any reduction in bias.</p>
<p>Therefore, if we wanted to count the number of of all possible samples (i.e. the volume of the moduli space of samples), we must not overcount the number of samples by individually counting samples differing by a permutation. In other words, every meaningful feature</p>
<p>One benefit of the notion of the empirical distribution is that it naturally avoids the quotient process. In other words, it’s a convenient representation of the moduli space of samples.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The conciseness of this construction highlights the utility of thinking about a sample as a map. Moreover, different choices for the distribution on <span class="math notranslate nohighlight">\(I\)</span> yield different “class weights” for the learning problem, and are the basis for boosting algorithms.</p>
</div>
<p>For completeness, we introduce some standard, informal terms</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>We call <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> the <em>space of features</em>, and <span class="math notranslate nohighlight">\(mathcal{Y}\)</span> the <em>space of labels</em></p>
</div>
</div>
<div class="section" id="models-and-hypotheses">
<h2>Models and Hypotheses<a class="headerlink" href="#models-and-hypotheses" title="Permalink to this headline">¶</a></h2>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p><em>The</em> Hypothesis Set:</p>
</div>
<ul class="simple">
<li><p>discriminative learning problem</p></li>
<li><p>generative learning problem</p></li>
</ul>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p><em>A</em> Discriminative Model Space</p>
<p>We call <span class="math notranslate nohighlight">\(\tilde{f}\)</span> the  model’s <em>underlying hypothesis</em>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This map need not be an embedding. For example, when the model space is the space of weights for a neural network with at least one hidden layer due to weight space symmetries.</p>
</div>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>Logistic Regression modelsspace/hypothesis space</p>
</div>
</div>
<div class="section" id="loss-functions">
<h2>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<p>A loss function is a pairing between data and models. In other words, it’s a map:</p>
<div class="admonition-definition-a-loss-function-for-a-learning-algorithm-math admonition">
<p class="admonition-title">Definition
A loss function for a learning algorithm
.. math::</p>
<blockquote>
<div><p>mathrm{Prob}(mathcal{X}timesmathcal{Y})times mathcal{M} &amp;overset{mathscr{L}}longrightarrow mathbb{R} \
(rho, f) &amp;longmapsto mathscr{L}_rho (f)</p>
</div></blockquote>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One might want to say that a loss function is a function of the hypothesis space. This would exlude the possibility of additional regularization terms to the loss function.</p>
</div>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>Information theory gives powerful loss functions in the form of relative entropy:</p>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Jeremy Mann's Writing</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../learning_main_page.html">Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../machine_learning/machine_learning_main_page.html">Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../statistical_inference/statistical_inference_main_page.html">Statistical Inference</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="statistical_learning_main_page.html">Statistical Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_science/data_science_main_page.html">Data Science</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../math/math_main_page.html">Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/physics_main_page.html">Physics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../longevity/longevity_main_page.html">Longevity</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../random/random_main_page.html">Random Writing</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../learning_main_page.html">Learning</a><ul>
  <li><a href="statistical_learning_main_page.html">Statistical Learning</a><ul>
      <li>Previous: <a href="statistical_learning_main_page.html" title="previous chapter">Statistical Learning</a></li>
      <li>Next: <a href="notions_of_error_in_machine_learning.html" title="next chapter">Notions and Interrelationships</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Jeremy Mann.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/learning/statistical_learning/fundamental_ingredients_of_statistical_learning.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>