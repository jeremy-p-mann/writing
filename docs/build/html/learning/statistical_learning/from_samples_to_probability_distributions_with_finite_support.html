
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>From Samples to Probability Distributions with Finite Support &#8212; Jeremy Mann&#39;s Writing 0.0.0 documentation</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Statistical Inference" href="../statistical_inference/statistical_inference_main_page.html" />
    <link rel="prev" title="Assessing Learners" href="assessing_learners.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="from-samples-to-probability-distributions-with-finite-support">
<h1>From Samples to Probability Distributions with Finite Support<a class="headerlink" href="#from-samples-to-probability-distributions-with-finite-support" title="Permalink to this headline">¶</a></h1>
<p>The goal of these notes are to show how to think of sample data as probability distributions satisfying a finite support condition.</p>
<div class="section" id="the-empirical-distribution">
<h2>The Empirical Distribution<a class="headerlink" href="#the-empirical-distribution" title="Permalink to this headline">¶</a></h2>
<p>Usually, sample data is thought of a bunch of tuples <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> for all <span class="math notranslate nohighlight">\(i=0, \ldots, N\)</span>.</p>
<p>In other words, it’s a map:</p>
<div class="math notranslate nohighlight">
\[\begin{split}I &amp;\overset{S}\longrightarrow \mathcal{X} \times \mathcal{Y} \\
i &amp;\longmapsto S_i =: (x_i, y_i)\end{split}\]</div>
<p>This factorization of data has a name:</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>We call <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> the <strong>space of features</strong>, and <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> the <strong>space of labels</strong>.</p>
</div>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>Let <span class="math notranslate nohighlight">\(\rho_I^\mathrm{unif}\)</span> denote the uniform distribution on the set <span class="math notranslate nohighlight">\(I\)</span>. Viewing the sample as a map allows us to efficiently generate a distribution on <span class="math notranslate nohighlight">\(\mathcal{X} \times \mathcal{Y}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\rho_S = S_* \rho_I\]</div>
<p>which we shall refer to as the <strong>empirical distribution</strong> associated to the sample <span class="math notranslate nohighlight">\(S\)</span>.</p>
</div>
<div class="admonition-motivation admonition">
<p class="admonition-title">Motivation</p>
<p>In practice, the ordering of a sample has any meaning in terms of the problem at hand. More concretely, no learning algorithm should depend on any ordering of the data, as any such dependence would introduce superfluous variance to the model. More importantly, this variance is not offest by any reduction in bias.</p>
<p>Therefore, if we wanted to count the number of of all possible samples (i.e. the volume of the moduli space of samples), we must not overcount the number of samples by individually counting samples differing by a permutation.</p>
<p>One benefit of the notion of the empirical distribution is that it naturally avoids the quotient process. In other words, it’s a convenient representation of the moduli space of samples.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The conciseness of this construction highlights the utility of thinking about a sample as a map. Moreover, different choices for the distribution on <span class="math notranslate nohighlight">\(I\)</span> yield different “class weights” for the learning problem, and are the basis for boosting algorithms.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In classical information theory, the empirical distribution is refered to as the <strong>type</strong> of the sample.</p>
</div>
</div>
<div class="section" id="probability-distributions-with-finite-support">
<h2>Probability Distributions with Finite Support<a class="headerlink" href="#probability-distributions-with-finite-support" title="Permalink to this headline">¶</a></h2>
<p>Below is a property which, in some sense, characterizes data probabilistically.</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>A <strong>probability distribution with finite support</strong> is a probability distribution of the form:</p>
<div class="math notranslate nohighlight">
\[(I \rightarrow \Omega)_*\tilde{\rho}\]</div>
<p>For some <span class="math notranslate nohighlight">\(S\)</span> and <span class="math notranslate nohighlight">\(\tilde{\rho}\in\mathrm{Prob}(I)\)</span>. In other words, only finitely many points have nonzero probability.</p>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Jeremy Mann's Writing</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../learning_main_page.html">Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../machine_learning/machine_learning_main_page.html">Machine Learning</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="statistical_learning_main_page.html">Statistical Learning</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="models_hypotheses_and_loss_functions.html">Models, Hypotheses and Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="assessing_learners.html">Assessing Learners</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">From Samples to Probability Distributions with Finite Support</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../statistical_inference/statistical_inference_main_page.html">Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_science/data_science_main_page.html">Data Science</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../math/math_main_page.html">Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../physics/physics_main_page.html">Physics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../longevity/longevity_main_page.html">Longevity</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../random/random_main_page.html">Random Writing</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../learning_main_page.html">Learning</a><ul>
  <li><a href="statistical_learning_main_page.html">Statistical Learning</a><ul>
      <li>Previous: <a href="assessing_learners.html" title="previous chapter">Assessing Learners</a></li>
      <li>Next: <a href="../statistical_inference/statistical_inference_main_page.html" title="next chapter">Statistical Inference</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Jeremy Mann.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/learning/statistical_learning/from_samples_to_probability_distributions_with_finite_support.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>